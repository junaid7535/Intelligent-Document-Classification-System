{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/01_eda.ipynb\n",
    "# ==============================================================================\n",
    "# Intelligent Document Classification System\n",
    "# Exploratory Data Analysis Notebook\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Data Loading\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üìä Loading datasets...\")\n",
    "\n",
    "# Load data from multiple sources\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "val_df = pd.read_csv('../data/raw/val.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "\n",
    "# Combine for overall analysis\n",
    "all_data = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Basic Data Exploration\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüîç Dataset Overview:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total documents: {len(all_data):,}\")\n",
    "print(f\"Number of features: {all_data.shape[1]}\")\n",
    "print(f\"Columns: {list(all_data.columns)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìÑ Sample documents:\")\n",
    "display(all_data.head())\n",
    "\n",
    "# Data types\n",
    "print(\"\\nüìà Data Types:\")\n",
    "print(all_data.dtypes)\n",
    "\n",
    "# Missing values\n",
    "print(\"\\n‚ùå Missing Values Analysis:\")\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': all_data.columns,\n",
    "    'Missing_Values': all_data.isnull().sum(),\n",
    "    'Missing_Percentage': (all_data.isnull().sum() / len(all_data)) * 100\n",
    "})\n",
    "print(missing_data[missing_data['Missing_Values'] > 0])\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Target Variable Analysis\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüéØ Target Variable Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if we have a target column\n",
    "target_col = 'category'  # Adjust based on your data\n",
    "if target_col in all_data.columns:\n",
    "    # Class distribution\n",
    "    class_dist = all_data[target_col].value_counts()\n",
    "    \n",
    "    print(f\"Number of unique classes: {len(class_dist)}\")\n",
    "    print(f\"Class distribution:\\n{class_dist}\")\n",
    "    \n",
    "    # Visualize class distribution\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Class Distribution', 'Class Proportions'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'pie'}]]\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=class_dist.index, y=class_dist.values,\n",
    "               marker_color=px.colors.qualitative.Set3,\n",
    "               text=class_dist.values,\n",
    "               textposition='auto'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=class_dist.index, values=class_dist.values,\n",
    "               hole=0.3,\n",
    "               marker_colors=px.colors.qualitative.Set3),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=500, showlegend=True,\n",
    "                      title_text=\"Target Class Distribution\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    imbalance_ratio = class_dist.max() / class_dist.min()\n",
    "    print(f\"\\n‚ö†Ô∏è Class imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "    if imbalance_ratio > 10:\n",
    "        print(\"Warning: Severe class imbalance detected!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Text Analysis\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüìù Text Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select text column\n",
    "text_col = 'text'  # Adjust based on your data\n",
    "if text_col in all_data.columns:\n",
    "    # Calculate text statistics\n",
    "    all_data['text_length'] = all_data[text_col].astype(str).apply(len)\n",
    "    all_data['word_count'] = all_data[text_col].astype(str).apply(lambda x: len(str(x).split()))\n",
    "    all_data['sentence_count'] = all_data[text_col].astype(str).apply(lambda x: len(str(x).split('.')))\n",
    "    all_data['avg_word_length'] = all_data[text_col].astype(str).apply(\n",
    "        lambda x: np.mean([len(word) for word in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Summary statistics\n",
    "    text_stats = all_data[['text_length', 'word_count', 'sentence_count', 'avg_word_length']].describe()\n",
    "    print(\"Text Statistics:\")\n",
    "    print(text_stats)\n",
    "    \n",
    "    # Visualize text length distributions\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Text Length Distribution', 'Word Count Distribution',\n",
    "                       'Sentence Count Distribution', 'Average Word Length'),\n",
    "        specs=[[{'type': 'histogram'}, {'type': 'histogram'}],\n",
    "               [{'type': 'histogram'}, {'type': 'histogram'}]]\n",
    "    )\n",
    "    \n",
    "    metrics = ['text_length', 'word_count', 'sentence_count', 'avg_word_length']\n",
    "    titles = ['Characters', 'Words', 'Sentences', 'Avg Word Length']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        row = i // 2 + 1\n",
    "        col = i % 2 + 1\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=all_data[metric],\n",
    "                        nbinsx=50,\n",
    "                        marker_color=px.colors.sequential.Blues[i*2],\n",
    "                        name=title),\n",
    "            row=row, col=col\n",
    "        )\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_val = all_data[metric].mean()\n",
    "        fig.add_vline(x=mean_val, line_dash=\"dash\", line_color=\"red\",\n",
    "                     annotation_text=f\"Mean: {mean_val:.1f}\",\n",
    "                     row=row, col=col)\n",
    "    \n",
    "    fig.update_layout(height=600, showlegend=False,\n",
    "                      title_text=\"Text Feature Distributions\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Text length by category\n",
    "    if target_col in all_data.columns:\n",
    "        fig = px.box(all_data, x=target_col, y='word_count',\n",
    "                    color=target_col,\n",
    "                    title=\"Word Count Distribution by Category\",\n",
    "                    labels={'word_count': 'Number of Words', target_col: 'Category'})\n",
    "        fig.update_layout(height=500)\n",
    "        fig.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. N-gram Analysis\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüî§ N-gram Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "if text_col in all_data.columns:\n",
    "    # Get stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Function to get top n-grams\n",
    "    def get_top_ngrams(corpus, ngram_range=(1,1), n=10):\n",
    "        vec = CountVectorizer(ngram_range=ngram_range, \n",
    "                            stop_words='english',\n",
    "                            max_features=10000).fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) \n",
    "                     for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    # Analyze for different n-grams\n",
    "    ngram_ranges = [(1,1), (2,2), (3,3)]\n",
    "    ngram_names = ['Unigrams', 'Bigrams', 'Trigrams']\n",
    "    \n",
    "    for (ngram_min, ngram_max), name in zip(ngram_ranges, ngram_names):\n",
    "        top_ngrams = get_top_ngrams(all_data[text_col].astype(str), \n",
    "                                   ngram_range=(ngram_min, ngram_max), \n",
    "                                   n=15)\n",
    "        \n",
    "        print(f\"\\nTop 15 {name}:\")\n",
    "        for word, freq in top_ngrams:\n",
    "            print(f\"  {word}: {freq}\")\n",
    "        \n",
    "        # Visualize\n",
    "        words, frequencies = zip(*top_ngrams)\n",
    "        \n",
    "        fig = px.bar(x=list(words)[::-1], y=list(frequencies)[::-1],\n",
    "                    orientation='h',\n",
    "                    title=f\"Top 15 {name}\",\n",
    "                    labels={'x': 'Frequency', 'y': f'{name}'},\n",
    "                    color=list(frequencies)[::-1],\n",
    "                    color_continuous_scale='Viridis')\n",
    "        fig.update_layout(height=400)\n",
    "        fig.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Category-Wise Analysis\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüìä Category-Wise Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if target_col in all_data.columns and text_col in all_data.columns:\n",
    "    # Get top words per category\n",
    "    unique_categories = all_data[target_col].unique()\n",
    "    \n",
    "    for category in unique_categories[:5]:  # Limit to first 5 categories for brevity\n",
    "        category_docs = all_data[all_data[target_col] == category][text_col]\n",
    "        \n",
    "        if len(category_docs) > 0:\n",
    "            top_words = get_top_ngrams(category_docs.astype(str), ngram_range=(1,1), n=10)\n",
    "            \n",
    "            print(f\"\\nTop words for category '{category}':\")\n",
    "            words, freqs = zip(*top_words)\n",
    "            for word, freq in zip(words, freqs):\n",
    "                print(f\"  {word}: {freq}\")\n",
    "            \n",
    "            # Word cloud for each category\n",
    "            try:\n",
    "                from wordcloud import WordCloud\n",
    "                \n",
    "                text = ' '.join(category_docs.astype(str))\n",
    "                wordcloud = WordCloud(width=800, height=400,\n",
    "                                    background_color='white',\n",
    "                                    max_words=50).generate(text)\n",
    "                \n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Word Cloud for '{category}'\")\n",
    "                plt.show()\n",
    "            except:\n",
    "                print(\"WordCloud not available. Install with: pip install wordcloud\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. Data Quality Analysis\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüîç Data Quality Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = all_data.duplicated(subset=[text_col] if text_col in all_data.columns else None).sum()\n",
    "print(f\"Duplicate documents: {duplicates} ({duplicates/len(all_data)*100:.2f}%)\")\n",
    "\n",
    "# Check for empty documents\n",
    "empty_docs = all_data[text_col].apply(lambda x: len(str(x).strip()) == 0).sum()\n",
    "print(f\"Empty documents: {empty_docs} ({empty_docs/len(all_data)*100:.2f}%)\")\n",
    "\n",
    "# Check for very short documents\n",
    "short_docs = all_data['word_count' if 'word_count' in all_data.columns else text_col].apply(\n",
    "    lambda x: len(str(x).split()) < 10\n",
    ").sum()\n",
    "print(f\"Very short documents (<10 words): {short_docs} ({short_docs/len(all_data)*100:.2f}%)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. Correlation Analysis\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüìà Correlation Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate correlations between text features\n",
    "if all('word_count' in all_data.columns,\n",
    "       'text_length' in all_data.columns,\n",
    "       'sentence_count' in all_data.columns):\n",
    "    \n",
    "    numeric_features = ['text_length', 'word_count', 'sentence_count', 'avg_word_length']\n",
    "    correlation_matrix = all_data[numeric_features].corr()\n",
    "    \n",
    "    fig = px.imshow(correlation_matrix,\n",
    "                   text_auto=True,\n",
    "                   aspect=\"auto\",\n",
    "                   color_continuous_scale='RdBu',\n",
    "                   title=\"Correlation Matrix of Text Features\")\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nFeature Correlations:\")\n",
    "    print(correlation_matrix)\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. Temporal Analysis (if date column exists)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüìÖ Temporal Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for date columns\n",
    "date_columns = [col for col in all_data.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "if date_columns:\n",
    "    date_col = date_columns[0]\n",
    "    all_data[date_col] = pd.to_datetime(all_data[date_col], errors='coerce')\n",
    "    \n",
    "    # Documents over time\n",
    "    docs_over_time = all_data[date_col].dt.to_period('M').value_counts().sort_index()\n",
    "    \n",
    "    fig = px.line(x=docs_over_time.index.astype(str), \n",
    "                 y=docs_over_time.values,\n",
    "                 title=\"Documents Over Time\",\n",
    "                 labels={'x': 'Month', 'y': 'Number of Documents'})\n",
    "    fig.update_layout(height=400)\n",
    "    fig.show()\n",
    "    \n",
    "    # Category distribution over time\n",
    "    if target_col in all_data.columns:\n",
    "        monthly_data = all_data.groupby([all_data[date_col].dt.to_period('M'), target_col]).size().unstack(fill_value=0)\n",
    "        \n",
    "        fig = px.line(monthly_data,\n",
    "                     title=\"Category Distribution Over Time\",\n",
    "                     labels={'value': 'Number of Documents', 'variable': 'Category'})\n",
    "        fig.update_layout(height=500)\n",
    "        fig.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. Summary and Recommendations\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüìã EDA Summary and Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "summary = {\n",
    "    \"Total Documents\": len(all_data),\n",
    "    \"Number of Classes\": len(all_data[target_col].unique()) if target_col in all_data.columns else 0,\n",
    "    \"Class Imbalance Ratio\": f\"{imbalance_ratio:.2f}\" if 'imbalance_ratio' in locals() else \"N/A\",\n",
    "    \"Average Word Count\": f\"{all_data['word_count'].mean():.1f}\" if 'word_count' in all_data.columns else \"N/A\",\n",
    "    \"Missing Values\": f\"{missing_data['Missing_Values'].sum()} total\",\n",
    "    \"Duplicate Documents\": duplicates,\n",
    "    \"Empty Documents\": empty_docs\n",
    "}\n",
    "\n",
    "for key, value in summary.items():\n",
    "    print(f\"‚úì {key}: {value}\")\n",
    "\n",
    "print(\"\\nüéØ Key Insights & Recommendations:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Class imbalance check\n",
    "if 'imbalance_ratio' in locals() and imbalance_ratio > 3:\n",
    "    insights.append(\"üî∏ Class imbalance detected - consider using class weights or oversampling/undersampling\")\n",
    "else:\n",
    "    insights.append(\"‚úÖ Class distribution is relatively balanced\")\n",
    "\n",
    "# Text length insights\n",
    "if 'word_count' in all_data.columns:\n",
    "    avg_words = all_data['word_count'].mean()\n",
    "    if avg_words < 50:\n",
    "        insights.append(\"üî∏ Documents are very short - consider using character-level models or n-grams\")\n",
    "    elif avg_words > 1000:\n",
    "        insights.append(\"üî∏ Documents are very long - consider truncation or hierarchical models\")\n",
    "    else:\n",
    "        insights.append(\"‚úÖ Document length is suitable for most models\")\n",
    "\n",
    "# Data quality\n",
    "if duplicates > 0:\n",
    "    insights.append(f\"üî∏ {duplicates} duplicates found - consider removing them\")\n",
    "if empty_docs > 0:\n",
    "    insights.append(f\"üî∏ {empty_docs} empty documents found - consider removing them\")\n",
    "\n",
    "# Missing values\n",
    "if missing_data['Missing_Values'].sum() > 0:\n",
    "    insights.append(\"üî∏ Missing values found - need to handle them in preprocessing\")\n",
    "\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "print(\"\\nüìä Next Steps:\")\n",
    "print(\"1. Address data quality issues (duplicates, empty docs)\")\n",
    "print(\"2. Handle class imbalance if significant\")\n",
    "print(\"3. Preprocess text data (cleaning, normalization)\")\n",
    "print(\"4. Split data appropriately for training\")\n",
    "print(\"5. Extract features based on text characteristics\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
