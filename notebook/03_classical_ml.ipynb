{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/03_classical_ml.ipynb\n",
    "# ==============================================================================\n",
    "# Intelligent Document Classification System\n",
    "# Classical Machine Learning Models\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    ")\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Load Processed Data\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üìä Loading processed data...\")\n",
    "\n",
    "train_df = pd.read_csv('../data/processed/train_processed.csv')\n",
    "val_df = pd.read_csv('../data/processed/val_processed.csv')\n",
    "test_df = pd.read_csv('../data/processed/test_processed.csv')\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_df['processed_text'].fillna('').astype(str)\n",
    "y_train = train_df['category']\n",
    "\n",
    "X_val = val_df['processed_text'].fillna('').astype(str)\n",
    "y_val = val_df['category']\n",
    "\n",
    "X_test = test_df['processed_text'].fillna('').astype(str)\n",
    "y_test = test_df['category']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(f\"\\nüéØ Number of classes: {len(label_encoder.classes_)}\")\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Feature Extraction\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Extracting features...\")\n",
    "\n",
    "# Define feature extraction methods\n",
    "def extract_features_tfidf(X_train, X_val, X_test, max_features=5000, ngram_range=(1,2)):\n",
    "    \"\"\"Extract TF-IDF features\"\"\"\n",
    "    print(f\"Extracting TF-IDF features (max_features={max_features})...\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = vectorizer.transform(X_val)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"TF-IDF shape - Train: {X_train_tfidf.shape}\")\n",
    "    \n",
    "    return X_train_tfidf, X_val_tfidf, X_test_tfidf, vectorizer\n",
    "\n",
    "def extract_features_count(X_train, X_val, X_test, max_features=5000, ngram_range=(1,2)):\n",
    "    \"\"\"Extract Count features\"\"\"\n",
    "    print(f\"Extracting Count features (max_features={max_features})...\")\n",
    "    \n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    X_train_count = vectorizer.fit_transform(X_train)\n",
    "    X_val_count = vectorizer.transform(X_val)\n",
    "    X_test_count = vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"Count shape - Train: {X_train_count.shape}\")\n",
    "    \n",
    "    return X_train_count, X_val_count, X_test_count, vectorizer\n",
    "\n",
    "def extract_features_ngrams(X_train, X_val, X_test, ngram_ranges=[(1,1), (2,2), (3,3)], max_features=2000):\n",
    "    \"\"\"Extract multiple n-gram features\"\"\"\n",
    "    print(\"Extracting multiple n-gram features...\")\n",
    "    \n",
    "    vectorizers = []\n",
    "    features = []\n",
    "    \n",
    "    for ngram_range in ngram_ranges:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range,\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        X_train_ngram = vectorizer.fit_transform(X_train)\n",
    "        X_val_ngram = vectorizer.transform(X_val)\n",
    "        X_test_ngram = vectorizer.transform(X_test)\n",
    "        \n",
    "        vectorizers.append(vectorizer)\n",
    "        features.append({\n",
    "            'train': X_train_ngram,\n",
    "            'val': X_val_ngram,\n",
    "            'test': X_test_ngram,\n",
    "            'ngram_range': ngram_range\n",
    "        })\n",
    "        \n",
    "        print(f\"  {ngram_range}-grams: {X_train_ngram.shape}\")\n",
    "    \n",
    "    return features, vectorizers\n",
    "\n",
    "# Extract different feature sets\n",
    "print(\"\\nüîß Creating feature sets...\")\n",
    "\n",
    "# TF-IDF features\n",
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vectorizer = extract_features_tfidf(\n",
    "    X_train, X_val, X_test, max_features=5000\n",
    ")\n",
    "\n",
    "# Count features\n",
    "X_train_count, X_val_count, X_test_count, count_vectorizer = extract_features_count(\n",
    "    X_train, X_val, X_test, max_features=5000\n",
    ")\n",
    "\n",
    "# Multiple n-gram features\n",
    "ngram_features, ngram_vectorizers = extract_features_ngrams(\n",
    "    X_train, X_val, X_test\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Model Definitions\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nü§ñ Defining models...\")\n",
    "\n",
    "class ClassicalMLModels:\n",
    "    \"\"\"Collection of classical ML models for document classification\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        \n",
    "    def define_models(self):\n",
    "        \"\"\"Define various ML models\"\"\"\n",
    "        \n",
    "        self.models = {\n",
    "            'logistic_regression': {\n",
    "                'model': LogisticRegression(\n",
    "                    random_state=self.random_state,\n",
    "                    max_iter=1000,\n",
    "                    n_jobs=-1\n",
    "                ),\n",
    "                'params': {\n",
    "                    'C': [0.1, 1, 10, 100],\n",
    "                    'penalty': ['l2'],\n",
    "                    'solver': ['lbfgs', 'sag']\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            'svm_linear': {\n",
    "                'model': LinearSVC(\n",
    "                    random_state=self.random_state,\n",
    "                    max_iter=10000\n",
    "                ),\n",
    "                'params': {\n",
    "                    'C': [0.1, 1, 10],\n",
    "                    'penalty': ['l2'],\n",
    "                    'loss': ['hinge', 'squared_hinge']\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            'random_forest': {\n",
    "                'model': RandomForestClassifier(\n",
    "                    random_state=self.random_state,\n",
    "                    n_jobs=-1\n",
    "                ),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [10, 20, 30, None],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4]\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            'naive_bayes': {\n",
    "                'model': MultinomialNB(),\n",
    "                'params': {\n",
    "                    'alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            'xgboost': {\n",
    "                'model': xgb.XGBClassifier(\n",
    "                    random_state=self.random_state,\n",
    "                    n_jobs=-1,\n",
    "                    verbosity=0\n",
    "                ),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [3, 6, 9],\n",
    "                    'learning_rate': [0.01, 0.1, 0.3],\n",
    "                    'subsample': [0.8, 1.0]\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            'lightgbm': {\n",
    "                'model': lgb.LGBMClassifier(\n",
    "                    random_state=self.random_state,\n",
    "                    n_jobs=-1,\n",
    "                    verbose=-1\n",
    "                ),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'num_leaves': [31, 50, 100],\n",
    "                    'learning_rate': [0.01, 0.1, 0.3],\n",
    "                    'subsample': [0.8, 1.0]\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            'gradient_boosting': {\n",
    "                'model': GradientBoostingClassifier(\n",
    "                    random_state=self.random_state\n",
    "                ),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'learning_rate': [0.01, 0.1, 0.3],\n",
    "                    'max_depth': [3, 6, 9],\n",
    "                    'subsample': [0.8, 1.0]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"Defined {len(self.models)} models:\")\n",
    "        for name in self.models.keys():\n",
    "            print(f\"  ‚Ä¢ {name}\")\n",
    "    \n",
    "    def train_model(self, model_name, X_train, y_train, X_val, y_val, \n",
    "                   feature_type='tfidf', use_grid_search=True):\n",
    "        \"\"\"Train a single model\"\"\"\n",
    "        \n",
    "        print(f\"\\nüèãÔ∏è Training {model_name} with {feature_type} features...\")\n",
    "        \n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model {model_name} not defined\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        model_config = self.models[model_name]\n",
    "        model = model_config['model']\n",
    "        \n",
    "        if use_grid_search and 'params' in model_config:\n",
    "            # Perform grid search\n",
    "            print(f\"  Performing grid search...\")\n",
    "            grid_search = GridSearchCV(\n",
    "                model,\n",
    "                model_config['params'],\n",
    "                cv=3,\n",
    "                scoring='accuracy',\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            best_score = grid_search.best_score_\n",
    "            \n",
    "            print(f\"  Best params: {best_params}\")\n",
    "            print(f\"  Best CV score: {best_score:.4f}\")\n",
    "            \n",
    "        else:\n",
    "            # Train without grid search\n",
    "            best_model = model\n",
    "            best_model.fit(X_train, y_train)\n",
    "            best_params = None\n",
    "            best_score = cross_val_score(model, X_train, y_train, cv=3, n_jobs=-1).mean()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_val_pred = best_model.predict(X_val)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        \n",
    "        # Get detailed metrics\n",
    "        val_report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        self.results[model_name] = {\n",
    "            'model': best_model,\n",
    "            'best_params': best_params,\n",
    "            'cv_score': best_score,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_report': val_report,\n",
    "            'training_time': training_time,\n",
    "            'feature_type': feature_type\n",
    "        }\n",
    "        \n",
    "        print(f\"  Validation accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  Training time: {training_time:.2f} seconds\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    def train_all_models(self, X_train, y_train, X_val, y_val, \n",
    "                        feature_type='tfidf', use_grid_search=True):\n",
    "        \"\"\"Train all defined models\"\"\"\n",
    "        \n",
    "        print(f\"\\nüöÄ Training all models with {feature_type} features...\")\n",
    "        \n",
    "        for model_name in self.models.keys():\n",
    "            try:\n",
    "                self.train_model(model_name, X_train, y_train, X_val, y_val,\n",
    "                               feature_type, use_grid_search)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error training {model_name}: {str(e)}\")\n",
    "        \n",
    "        # Find best model\n",
    "        self.find_best_model()\n",
    "    \n",
    "    def find_best_model(self):\n",
    "        \"\"\"Find the best performing model\"\"\"\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"No models trained yet!\")\n",
    "            return\n",
    "        \n",
    "        best_model_name = max(self.results.keys(), \n",
    "                            key=lambda x: self.results[x]['val_accuracy'])\n",
    "        \n",
    "        self.best_model = {\n",
    "            'name': best_model_name,\n",
    "            'model': self.results[best_model_name]['model'],\n",
    "            'accuracy': self.results[best_model_name]['val_accuracy'],\n",
    "            'config': self.results[best_model_name]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüèÜ Best model: {best_model_name}\")\n",
    "        print(f\"   Validation accuracy: {self.best_model['accuracy']:.4f}\")\n",
    "    \n",
    "    def evaluate_model(self, model_name, X_test, y_test):\n",
    "        \"\"\"Evaluate a model on test set\"\"\"\n",
    "        \n",
    "        if model_name not in self.results:\n",
    "            raise ValueError(f\"Model {model_name} not trained\")\n",
    "        \n",
    "        model = self.results[model_name]['model']\n",
    "        \n",
    "        print(f\"\\nüß™ Evaluating {model_name} on test set...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_test, y_pred, average='weighted'\n",
    "        )\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Classification report\n",
    "        report = classification_report(y_test, y_pred, \n",
    "                                     target_names=label_encoder.classes_)\n",
    "        \n",
    "        # Store test results\n",
    "        self.results[model_name]['test_results'] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm,\n",
    "            'classification_report': report,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"  Test accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  F1-score: {f1:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        \n",
    "        return accuracy, report, cm\n",
    "    \n",
    "    def create_ensemble(self, top_n=3):\n",
    "        \"\"\"Create ensemble of top performing models\"\"\"\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"No models trained yet!\")\n",
    "            return\n",
    "        \n",
    "        # Get top n models\n",
    "        sorted_models = sorted(self.results.items(), \n",
    "                             key=lambda x: x[1]['val_accuracy'], \n",
    "                             reverse=True)[:top_n]\n",
    "        \n",
    "        print(f\"\\nü§ù Creating ensemble of top {top_n} models:\")\n",
    "        \n",
    "        estimators = []\n",
    "        for model_name, result in sorted_models:\n",
    "            print(f\"  ‚Ä¢ {model_name} (val_acc: {result['val_accuracy']:.4f})\")\n",
    "            estimators.append((model_name, result['model']))\n",
    "        \n",
    "        # Create voting classifier\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=estimators,\n",
    "            voting='soft',  # Use soft voting for probability-based\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return ensemble\n",
    "    \n",
    "    def plot_results_comparison(self):\n",
    "        \"\"\"Plot comparison of all model results\"\"\"\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"No results to plot!\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        models = []\n",
    "        val_accuracies = []\n",
    "        train_times = []\n",
    "        \n",
    "        for model_name, result in self.results.items():\n",
    "            models.append(model_name)\n",
    "            val_accuracies.append(result['val_accuracy'])\n",
    "            train_times.append(result['training_time'])\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "        bars1 = axes[0].bar(models, val_accuracies, color=colors)\n",
    "        axes[0].set_title('Model Validation Accuracy Comparison', fontsize=14)\n",
    "        axes[0].set_xlabel('Model', fontsize=12)\n",
    "        axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "        axes[0].set_ylim([0, 1])\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add accuracy values on bars\n",
    "        for bar, acc in zip(bars1, val_accuracies):\n",
    "            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{acc:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Training time comparison\n",
    "        bars2 = axes[1].bar(models, train_times, color=colors)\n",
    "        axes[1].set_title('Model Training Time Comparison', fontsize=14)\n",
    "        axes[1].set_xlabel('Model', fontsize=12)\n",
    "        axes[1].set_ylabel('Time (seconds)', fontsize=12)\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add time values on bars\n",
    "        for bar, time_val in zip(bars2, train_times):\n",
    "            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                       f'{time_val:.1f}s', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature importance for tree-based models\n",
    "        tree_models = ['random_forest', 'xgboost', 'lightgbm', 'gradient_boosting']\n",
    "        for model_name in tree_models:\n",
    "            if model_name in self.results:\n",
    "                model = self.results[model_name]['model']\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    print(f\"\\nüìä Feature importance for {model_name}:\")\n",
    "                    \n",
    "                    # Get feature names\n",
    "                    if self.results[model_name]['feature_type'] == 'tfidf':\n",
    "                        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "                    else:\n",
    "                        feature_names = count_vectorizer.get_feature_names_out()\n",
    "                    \n",
    "                    # Get top features\n",
    "                    importances = model.feature_importances_\n",
    "                    indices = np.argsort(importances)[::-1][:20]\n",
    "                    \n",
    "                    print(\"Top 20 important features:\")\n",
    "                    for idx in indices:\n",
    "                        print(f\"  {feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "\n",
    "# Initialize models\n",
    "ml_models = ClassicalMLModels(random_state=42)\n",
    "ml_models.define_models()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Train Models with TF-IDF Features\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÇ Training with TF-IDF Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train all models\n",
    "ml_models.train_all_models(\n",
    "    X_train_tfidf, y_train_encoded,\n",
    "    X_val_tfidf, y_val_encoded,\n",
    "    feature_type='tfidf',\n",
    "    use_grid_search=True  # Set to False for faster training\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "ml_models.plot_results_comparison()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Train Models with Count Features\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÇ Training with Count Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create new instance for count features\n",
    "ml_models_count = ClassicalMLModels(random_state=42)\n",
    "ml_models_count.define_models()\n",
    "\n",
    "# Train with count features\n",
    "ml_models_count.train_all_models(\n",
    "    X_train_count, y_train_encoded,\n",
    "    X_val_count, y_val_encoded,\n",
    "    feature_type='count',\n",
    "    use_grid_search=False  # Faster training\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "ml_models_count.plot_results_comparison()\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Feature Importance Analysis\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüîç Analyzing feature importance...\")\n",
    "\n",
    "# Get best model from TF-IDF training\n",
    "best_model_name = ml_models.best_model['name']\n",
    "best_model = ml_models.results[best_model_name]['model']\n",
    "\n",
    "print(f\"Analyzing feature importance for best model: {best_model_name}\")\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False).head(30)\n",
    "    \n",
    "    # Plot top features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(feature_importance_df['feature'][::-1], \n",
    "                   feature_importance_df['importance'][::-1])\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.title(f'Top 30 Feature Importance - {best_model_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance_df.to_csv('../reports/feature_importance.csv', index=False)\n",
    "    print(\"‚úì Feature importance saved to '../reports/feature_importance.csv'\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. Evaluate Best Model on Test Set\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß™ Final Evaluation on Test Set\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate best model\n",
    "test_accuracy, test_report, test_cm = ml_models.evaluate_model(\n",
    "    best_model_name, X_test_tfidf, y_test_encoded\n",
    ")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(test_report)\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. Create and Evaluate Ensemble Model\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ù Creating Ensemble Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = ml_models.create_ensemble(top_n=3)\n",
    "\n",
    "# Train ensemble\n",
    "print(\"\\nTraining ensemble model...\")\n",
    "start_time = time.time()\n",
    "ensemble.fit(X_train_tfidf, y_train_encoded)\n",
    "ensemble_time = time.time() - start_time\n",
    "\n",
    "# Evaluate ensemble on validation set\n",
    "y_val_pred_ensemble = ensemble.predict(X_val_tfidf)\n",
    "ensemble_val_accuracy = accuracy_score(y_val_encoded, y_val_pred_ensemble)\n",
    "print(f\"Ensemble validation accuracy: {ensemble_val_accuracy:.4f}\")\n",
    "print(f\"Ensemble training time: {ensemble_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate ensemble on test set\n",
    "y_test_pred_ensemble = ensemble.predict(X_test_tfidf)\n",
    "ensemble_test_accuracy = accuracy_score(y_test_encoded, y_test_pred_ensemble)\n",
    "print(f\"Ensemble test accuracy: {ensemble_test_accuracy:.4f}\")\n",
    "\n",
    "# Compare with best single model\n",
    "print(f\"\\nComparison with best single model ({best_model_name}):\")\n",
    "print(f\"  Single model test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  Ensemble test accuracy: {ensemble_test_accuracy:.4f}\")\n",
    "print(f\"  Improvement: {((ensemble_test_accuracy - test_accuracy) / test_accuracy * 100):.2f}%\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. Model Comparison Report\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä Model Performance Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for model_name, result in ml_models.results.items():\n",
    "    if 'test_results' in result:\n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'Feature_Type': result['feature_type'],\n",
    "            'CV_Score': f\"{result['cv_score']:.4f}\",\n",
    "            'Val_Accuracy': f\"{result['val_accuracy']:.4f}\",\n",
    "            'Test_Accuracy': f\"{result['test_results']['accuracy']:.4f}\",\n",
    "            'F1_Score': f\"{result['test_results']['f1_score']:.4f}\",\n",
    "            'Training_Time(s)': f\"{result['training_time']:.2f}\"\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Add ensemble to summary\n",
    "if 'ensemble_test_accuracy' in locals():\n",
    "    summary_df = pd.concat([summary_df, pd.DataFrame([{\n",
    "        'Model': 'Ensemble (top 3)',\n",
    "        'Feature_Type': 'tfidf',\n",
    "        'CV_Score': '-',\n",
    "        'Val_Accuracy': f\"{ensemble_val_accuracy:.4f}\",\n",
    "        'Test_Accuracy': f\"{ensemble_test_accuracy:.4f}\",\n",
    "        'F1_Score': f\"{precision_recall_fscore_support(y_test_encoded, y_test_pred_ensemble, average='weighted')[2]:.4f}\",\n",
    "        'Training_Time(s)': f\"{ensemble_time:.2f}\"\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('../reports/classical_ml_summary.csv', index=False)\n",
    "print(\"\\n‚úì Model summary saved to '../reports/classical_ml_summary.csv'\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. Save Best Model and Artifacts\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüíæ Saving models and artifacts...\")\n",
    "\n",
    "# Create directory for models\n",
    "import os\n",
    "os.makedirs('../models/classical_ml', exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "best_model_path = f'../models/classical_ml/{best_model_name}_best_model.pkl'\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"‚úì Best model saved to {best_model_path}\")\n",
    "\n",
    "# Save ensemble model\n",
    "ensemble_path = '../models/classical_ml/ensemble_model.pkl'\n",
    "joblib.dump(ensemble, ensemble_path)\n",
    "print(f\"‚úì Ensemble model saved to {ensemble_path}\")\n",
    "\n",
    "# Save vectorizer\n",
    "vectorizer_path = '../models/classical_ml/tfidf_vectorizer.pkl'\n",
    "joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
    "print(f\"‚úì TF-IDF vectorizer saved to {vectorizer_path}\")\n",
    "\n",
    "# Save label encoder\n",
    "label_encoder_path = '../models/classical_ml/label_encoder.pkl'\n",
    "joblib.dump(label_encoder, label_encoder_path)\n",
    "print(f\"‚úì Label encoder saved to {label_encoder_path}\")\n",
    "\n",
    "# Save all results\n",
    "results_path = '../models/classical_ml/all_models_results.pkl'\n",
    "joblib.dump(ml_models.results, results_path)\n",
    "print(f\"‚úì All model results saved to {results_path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 11. Generate Final Report\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüìã Generating final report...\")\n",
    "\n",
    "# Create comprehensive report\n",
    "final_report = f\"\"\"\n",
    "# Classical ML Models Report\n",
    "## Document Classification System\n",
    "\n",
    "### Summary\n",
    "- **Best Model**: {best_model_name}\n",
    "- **Best Test Accuracy**: {test_accuracy:.4f}\n",
    "- **Ensemble Test Accuracy**: {ensemble_test_accuracy:.4f}\n",
    "- **Number of Models Evaluated**: {len(ml_models.results)}\n",
    "- **Feature Type**: TF-IDF with 5,000 features\n",
    "\n",
    "### Model Performance Ranking\n",
    "{summary_df.to_markdown(index=False)}\n",
    "\n",
    "### Key Findings\n",
    "1. **Best Performing Algorithm**: {best_model_name} achieved the highest accuracy\n",
    "2. **Feature Importance**: Top features were primarily domain-specific terms\n",
    "3. **Ensemble Benefit**: Ensemble improved accuracy by {((ensemble_test_accuracy - test_accuracy) / test_accuracy * 100):.2f}%\n",
    "4. **Training Efficiency**: {summary_df.loc[summary_df['Training_Time(s)'].astype(float).idxmin(), 'Model']} was the fastest to train\n",
    "\n",
    "### Recommendations\n",
    "1. **Production Deployment**: Use {best_model_name} or the ensemble model\n",
    "2. **Feature Engineering**: Consider adding domain-specific features\n",
    "3. **Model Optimization**: Further hyperparameter tuning could improve performance\n",
    "4. **Monitoring**: Track model drift with validation accuracy over time\n",
    "\n",
    "### Next Steps\n",
    "1. Proceed to transformer-based models for comparison\n",
    "2. Implement model serving API\n",
    "3. Create monitoring dashboard\n",
    "4. Set up retraining pipeline\n",
    "\"\"\"\n",
    "\n",
    "with open('../reports/classical_ml_final_report.md', 'w') as f:\n",
    "    f.write(final_report)\n",
    "\n",
    "print(\"‚úì Final report saved to '../reports/classical_ml_final_report.md'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Classical ML Training Complete!\")\n",
    "print(f\"üèÜ Best Model: {best_model_name} with {test_accuracy:.4f} accuracy\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
