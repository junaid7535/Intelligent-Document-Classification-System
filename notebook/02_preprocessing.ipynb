{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/02_preprocessing.ipynb\n",
    "# ==============================================================================\n",
    "# Intelligent Document Classification System\n",
    "# Preprocessing Pipeline Notebook\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources\n",
    "print(\"üì• Downloading NLTK resources...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-eng', quiet=True)\n",
    "\n",
    "# Load spaCy model\n",
    "print(\"üì• Loading spaCy model...\")\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'])\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Configuration & Settings\n",
    "# ==============================================================================\n",
    "\n",
    "class PreprocessingConfig:\n",
    "    \"\"\"Configuration for preprocessing pipeline\"\"\"\n",
    "    \n",
    "    # Text cleaning\n",
    "    REMOVE_SPECIAL_CHARS = True\n",
    "    REMOVE_DIGITS = False\n",
    "    CONVERT_TO_LOWERCASE = True\n",
    "    REMOVE_EXTRA_SPACES = True\n",
    "    \n",
    "    # Stopwords\n",
    "    REMOVE_STOPWORDS = True\n",
    "    CUSTOM_STOPWORDS = ['example', 'document', 'file', 'page']\n",
    "    \n",
    "    # Lemmatization/Stemming\n",
    "    LEMMATIZE = True\n",
    "    STEM = False\n",
    "    \n",
    "    # Special processing\n",
    "    REMOVE_EMAIL = True\n",
    "    REMOVE_URL = True\n",
    "    REMOVE_HTML = True\n",
    "    EXPAND_CONTRACTIONS = True\n",
    "    \n",
    "    # Text length management\n",
    "    MIN_TOKEN_LENGTH = 2\n",
    "    MAX_TOKEN_LENGTH = 100\n",
    "    MAX_DOCUMENT_LENGTH = 5000  # Truncate longer documents\n",
    "    \n",
    "    # Tokenization\n",
    "    TOKENIZER = 'spacy'  # Options: 'spacy', 'nltk'\n",
    "\n",
    "config = PreprocessingConfig()\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Load Data\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üìä Loading data...\")\n",
    "\n",
    "# Load data from EDA phase\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "val_df = pd.read_csv('../data/raw/val.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "# Combine for preprocessing consistency\n",
    "all_data = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "print(f\"Total documents: {len(all_data):,}\")\n",
    "print(f\"Columns: {list(all_data.columns)}\")\n",
    "\n",
    "# Identify text and target columns\n",
    "text_col = 'text'  # Update based on your dataset\n",
    "target_col = 'category'  # Update based on your dataset\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Preprocessing Class Definition\n",
    "# ==============================================================================\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    \"\"\"Complete document preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stop_words.update(config.CUSTOM_STOPWORDS)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        # Contraction mapping\n",
    "        self.contraction_map = {\n",
    "            \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
    "            \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "            \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "            \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
    "            \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "            \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\",\n",
    "            \"how's\": \"how is\", \"i'd\": \"i would\", \"i'll\": \"i will\",\n",
    "            \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\",\n",
    "            \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\",\n",
    "            \"let's\": \"let us\", \"ma'am\": \"madam\", \"might've\": \"might have\",\n",
    "            \"mightn't\": \"might not\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
    "            \"needn't\": \"need not\", \"oughtn't\": \"ought not\", \"shan't\": \"shall not\",\n",
    "            \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n",
    "            \"should've\": \"should have\", \"shouldn't\": \"should not\", \"so've\": \"so have\",\n",
    "            \"that's\": \"that is\", \"there's\": \"there is\", \"they'd\": \"they would\",\n",
    "            \"they'll\": \"they will\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "            \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'll\": \"we will\",\n",
    "            \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "            \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "            \"what've\": \"what have\", \"where's\": \"where is\", \"who'll\": \"who will\",\n",
    "            \"who's\": \"who is\", \"won't\": \"will not\", \"would've\": \"would have\",\n",
    "            \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\",\n",
    "            \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        if self.config.CONVERT_TO_LOWERCASE:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        if self.config.REMOVE_HTML:\n",
    "            text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        if self.config.REMOVE_URL:\n",
    "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # Remove emails\n",
    "        if self.config.REMOVE_EMAIL:\n",
    "            text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Expand contractions\n",
    "        if self.config.EXPAND_CONTRACTIONS:\n",
    "            for contraction, expansion in self.contraction_map.items():\n",
    "                text = text.replace(contraction, expansion)\n",
    "        \n",
    "        # Remove special characters\n",
    "        if self.config.REMOVE_SPECIAL_CHARS:\n",
    "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        # Remove digits\n",
    "        if self.config.REMOVE_DIGITS:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        if self.config.REMOVE_EXTRA_SPACES:\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text, method='spacy'):\n",
    "        \"\"\"Tokenize text using specified method\"\"\"\n",
    "        if method == 'spacy':\n",
    "            doc = nlp(text)\n",
    "            tokens = [token.text for token in doc]\n",
    "        elif method == 'nltk':\n",
    "            tokens = word_tokenize(text)\n",
    "        else:\n",
    "            tokens = text.split()\n",
    "        return tokens\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords from tokens\"\"\"\n",
    "        if self.config.REMOVE_STOPWORDS:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        return tokens\n",
    "    \n",
    "    def lemmatize_tokens(self, tokens):\n",
    "        \"\"\"Lemmatize tokens\"\"\"\n",
    "        if self.config.LEMMATIZE:\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        \"\"\"Stem tokens\"\"\"\n",
    "        if self.config.STEM:\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def filter_tokens_by_length(self, tokens):\n",
    "        \"\"\"Filter tokens by length\"\"\"\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            if (self.config.MIN_TOKEN_LENGTH <= len(token) <= \n",
    "                self.config.MAX_TOKEN_LENGTH):\n",
    "                filtered.append(token)\n",
    "        return filtered\n",
    "    \n",
    "    def preprocess_document(self, text, return_as='string'):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline for a single document\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            return_as: 'string' or 'tokens'\n",
    "        \"\"\"\n",
    "        # Clean text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenize(cleaned_text, method=self.config.TOKENIZER)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        # Lemmatize or stem\n",
    "        if self.config.LEMMATIZE:\n",
    "            tokens = self.lemmatize_tokens(tokens)\n",
    "        elif self.config.STEM:\n",
    "            tokens = self.stem_tokens(tokens)\n",
    "        \n",
    "        # Filter by length\n",
    "        tokens = self.filter_tokens_by_length(tokens)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(tokens) > self.config.MAX_DOCUMENT_LENGTH:\n",
    "            tokens = tokens[:self.config.MAX_DOCUMENT_LENGTH]\n",
    "        \n",
    "        if return_as == 'string':\n",
    "            return ' '.join(tokens)\n",
    "        else:\n",
    "            return tokens\n",
    "    \n",
    "    def batch_preprocess(self, texts, return_as='string', n_jobs=-1):\n",
    "        \"\"\"Preprocess multiple documents\"\"\"\n",
    "        from tqdm import tqdm\n",
    "        tqdm.pandas()\n",
    "        \n",
    "        print(f\"Preprocessing {len(texts)} documents...\")\n",
    "        \n",
    "        # Use parallel processing for large datasets\n",
    "        if len(texts) > 1000 and n_jobs != 1:\n",
    "            from joblib import Parallel, delayed\n",
    "            results = Parallel(n_jobs=n_jobs)(\n",
    "                delayed(self.preprocess_document)(text, return_as)\n",
    "                for text in tqdm(texts, desc=\"Preprocessing\")\n",
    "            )\n",
    "        else:\n",
    "            results = [\n",
    "                self.preprocess_document(text, return_as)\n",
    "                for text in tqdm(texts, desc=\"Preprocessing\")\n",
    "            ]\n",
    "        \n",
    "        return results\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Initialize Preprocessor\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüõ†Ô∏è Initializing preprocessor...\")\n",
    "preprocessor = DocumentPreprocessor(config)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Sample Preprocessing\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüîç Sample preprocessing demonstration:\")\n",
    "\n",
    "# Get sample documents\n",
    "sample_texts = all_data[text_col].head(5).tolist()\n",
    "\n",
    "for i, text in enumerate(sample_texts[:3]):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sample {i+1} - Original:\")\n",
    "    print('-'*30)\n",
    "    print(text[:500] + \"...\" if len(text) > 500 else text)\n",
    "    \n",
    "    # Process the text\n",
    "    processed = preprocessor.preprocess_document(text, return_as='string')\n",
    "    \n",
    "    print(f\"\\nSample {i+1} - Processed:\")\n",
    "    print('-'*30)\n",
    "    print(processed[:500] + \"...\" if len(processed) > 500 else processed)\n",
    "    \n",
    "    # Show tokens\n",
    "    tokens = preprocessor.preprocess_document(text, return_as='tokens')\n",
    "    print(f\"\\nTokens ({len(tokens)}):\")\n",
    "    print(tokens[:20])\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Apply Preprocessing to Entire Dataset\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Applying preprocessing to entire dataset...\")\n",
    "\n",
    "# Create copies to avoid modifying originals\n",
    "train_processed = train_df.copy()\n",
    "val_processed = val_df.copy()\n",
    "test_processed = test_df.copy()\n",
    "\n",
    "# Apply preprocessing\n",
    "train_processed['processed_text'] = preprocessor.batch_preprocess(\n",
    "    train_df[text_col], return_as='string', n_jobs=4\n",
    ")\n",
    "\n",
    "val_processed['processed_text'] = preprocessor.batch_preprocess(\n",
    "    val_df[text_col], return_as='string', n_jobs=4\n",
    ")\n",
    "\n",
    "test_processed['processed_text'] = preprocessor.batch_preprocess(\n",
    "    test_df[text_col], return_as='string', n_jobs=4\n",
    ")\n",
    "\n",
    "# Also store tokenized version for some models\n",
    "print(\"\\nüìù Creating tokenized versions...\")\n",
    "train_processed['tokens'] = preprocessor.batch_preprocess(\n",
    "    train_df[text_col], return_as='tokens', n_jobs=4\n",
    ")\n",
    "val_processed['tokens'] = preprocessor.batch_preprocess(\n",
    "    val_df[text_col], return_as='tokens', n_jobs=4\n",
    ")\n",
    "test_processed['tokens'] = preprocessor.batch_preprocess(\n",
    "    test_df[text_col], return_as='tokens', n_jobs=4\n",
    ")\n",
    "\n",
    "# Calculate statistics\n",
    "train_processed['processed_length'] = train_processed['processed_text'].apply(len)\n",
    "train_processed['processed_word_count'] = train_processed['processed_text'].apply(\n",
    "    lambda x: len(x.split())\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
